{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('crawler/output.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://docs.nvidia.com/cuda/</td>\n",
       "      <td>CUDA Toolkit Documentation 12.5</td>\n",
       "      <td>NvidiaSearchInput.mount({\"apiUrl\": \"https://ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://docs.nvidia.com/cuda/cuda-toolkit-rele...</td>\n",
       "      <td>CUDA 12.5 Update 1 Release Notes</td>\n",
       "      <td>1. CUDA 12.5 Update 1 Release Notes \\r\\n 1.1. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://docs.nvidia.com/cuda/cuda-installation...</td>\n",
       "      <td>CUDA Installation Guide for Linux</td>\n",
       "      <td>1. Introduction \\n 1.1. System Requirements \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://docs.nvidia.com/cuda/cuda-features-arc...</td>\n",
       "      <td>CUDA Features Archive</td>\n",
       "      <td>1. CUDA 11.6 Features \\n \\n 1.1. Compiler \\n 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://docs.nvidia.com/cuda/cuda-installation...</td>\n",
       "      <td>CUDA Installation Guide for Microsoft Windows</td>\n",
       "      <td>1. Introduction \\n 1.1. System Requirements \\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0                      https://docs.nvidia.com/cuda/   \n",
       "1  https://docs.nvidia.com/cuda/cuda-toolkit-rele...   \n",
       "2  https://docs.nvidia.com/cuda/cuda-installation...   \n",
       "3  https://docs.nvidia.com/cuda/cuda-features-arc...   \n",
       "4  https://docs.nvidia.com/cuda/cuda-installation...   \n",
       "\n",
       "                                           title  \\\n",
       "0                CUDA Toolkit Documentation 12.5   \n",
       "1               CUDA 12.5 Update 1 Release Notes   \n",
       "2              CUDA Installation Guide for Linux   \n",
       "3                          CUDA Features Archive   \n",
       "4  CUDA Installation Guide for Microsoft Windows   \n",
       "\n",
       "                                             content  \n",
       "0  NvidiaSearchInput.mount({\"apiUrl\": \"https://ap...  \n",
       "1  1. CUDA 12.5 Update 1 Release Notes \\r\\n 1.1. ...  \n",
       "2  1. Introduction \\n 1.1. System Requirements \\n...  \n",
       "3  1. CUDA 11.6 Features \\n \\n 1.1. Compiler \\n 1...  \n",
       "4  1. Introduction \\n 1.1. System Requirements \\n...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>complete_texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://docs.nvidia.com/cuda/</td>\n",
       "      <td>CUDA Toolkit Documentation 12.5</td>\n",
       "      <td>NvidiaSearchInput.mount({\"apiUrl\": \"https://ap...</td>\n",
       "      <td>CUDA Toolkit Documentation 12.5\\nNvidiaSearchI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://docs.nvidia.com/cuda/cuda-toolkit-rele...</td>\n",
       "      <td>CUDA 12.5 Update 1 Release Notes</td>\n",
       "      <td>1. CUDA 12.5 Update 1 Release Notes \\r\\n 1.1. ...</td>\n",
       "      <td>CUDA 12.5 Update 1 Release Notes\\n1. CUDA 12.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://docs.nvidia.com/cuda/cuda-installation...</td>\n",
       "      <td>CUDA Installation Guide for Linux</td>\n",
       "      <td>1. Introduction \\n 1.1. System Requirements \\n...</td>\n",
       "      <td>CUDA Installation Guide for Linux\\n1. Introduc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://docs.nvidia.com/cuda/cuda-features-arc...</td>\n",
       "      <td>CUDA Features Archive</td>\n",
       "      <td>1. CUDA 11.6 Features \\n \\n 1.1. Compiler \\n 1...</td>\n",
       "      <td>CUDA Features Archive\\n1. CUDA 11.6 Features \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://docs.nvidia.com/cuda/cuda-installation...</td>\n",
       "      <td>CUDA Installation Guide for Microsoft Windows</td>\n",
       "      <td>1. Introduction \\n 1.1. System Requirements \\n...</td>\n",
       "      <td>CUDA Installation Guide for Microsoft Windows\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0                      https://docs.nvidia.com/cuda/   \n",
       "1  https://docs.nvidia.com/cuda/cuda-toolkit-rele...   \n",
       "2  https://docs.nvidia.com/cuda/cuda-installation...   \n",
       "3  https://docs.nvidia.com/cuda/cuda-features-arc...   \n",
       "4  https://docs.nvidia.com/cuda/cuda-installation...   \n",
       "\n",
       "                                           title  \\\n",
       "0                CUDA Toolkit Documentation 12.5   \n",
       "1               CUDA 12.5 Update 1 Release Notes   \n",
       "2              CUDA Installation Guide for Linux   \n",
       "3                          CUDA Features Archive   \n",
       "4  CUDA Installation Guide for Microsoft Windows   \n",
       "\n",
       "                                             content  \\\n",
       "0  NvidiaSearchInput.mount({\"apiUrl\": \"https://ap...   \n",
       "1  1. CUDA 12.5 Update 1 Release Notes \\r\\n 1.1. ...   \n",
       "2  1. Introduction \\n 1.1. System Requirements \\n...   \n",
       "3  1. CUDA 11.6 Features \\n \\n 1.1. Compiler \\n 1...   \n",
       "4  1. Introduction \\n 1.1. System Requirements \\n...   \n",
       "\n",
       "                                      complete_texts  \n",
       "0  CUDA Toolkit Documentation 12.5\\nNvidiaSearchI...  \n",
       "1  CUDA 12.5 Update 1 Release Notes\\n1. CUDA 12.5...  \n",
       "2  CUDA Installation Guide for Linux\\n1. Introduc...  \n",
       "3  CUDA Features Archive\\n1. CUDA 11.6 Features \\...  \n",
       "4  CUDA Installation Guide for Microsoft Windows\\...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"complete_texts\"] = df.apply(lambda x: str(x[\"title\"])+\"\\n\"+str(x[\"content\"]), axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4856"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = df.complete_texts.to_list()\n",
    "\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Toolkit Documentation 12.5\n",
      "NvidiaSearchInput.mount({\"apiUrl\": \"https://api-prod.nvidia.com/search/graphql\", \"destination\": \"search.html\", \"path\": \"/cuda/\", \"site\": \"https://docs.nvidia.com\"});\n",
      " \n",
      "         \n",
      "               \n",
      " Release Notes \n",
      " CUDA Features Archive \n",
      " EULA \n",
      " \n",
      " Installation Guides \n",
      " \n",
      " Quick Start Guide \n",
      " Installation Guide Windows \n",
      " Installation Guide Linux \n",
      " \n",
      " Programming Guides \n",
      " \n",
      " Programming Guide \n",
      " Best Practices Guide \n",
      " Maxwell Compatibility Guide \n",
      " Pascal \n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0].complete_texts[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_string(s):\n",
    "    cleaned_lines = [' '.join(line.split()) for line in s.splitlines()]\n",
    "    cleaned_string = '\\n'.join(cleaned_lines)\n",
    "    cleaned_string = re.sub(r'\\n+', '\\n', cleaned_string)\n",
    "    return cleaned_string\n",
    "\n",
    "df[\"complete_texts\"] = df[\"complete_texts\"].apply(clean_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Toolkit Documentation 12.5\n",
      "NvidiaSearchInput.mount({\"apiUrl\": \"https://api-prod.nvidia.com/search/graphql\", \"destination\": \"search.html\", \"path\": \"/cuda/\", \"site\": \"https://docs.nvidia.com\"});\n",
      "Release Notes\n",
      "CUDA Features Archive\n",
      "EULA\n",
      "Installation Guides\n",
      "Quick Start Guide\n",
      "Installation Guide Windows\n",
      "Installation Guide Linux\n",
      "Programming Guides\n",
      "Programming Guide\n",
      "Best Practices Guide\n",
      "Maxwell Compatibility Guide\n",
      "Pascal Compatibility Guide\n",
      "Volta Compatibility Guide\n",
      "Turing Compatibility Guide\n",
      "NVIDI\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0].complete_texts[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import json\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings()\n",
    "text_splitter = SemanticChunker(hf_embeddings)\n",
    "\n",
    "with open(\"chunked_text.jsonl\", \"a\") as f:\n",
    "    for i in range(len(df)):\n",
    "        text = df.iloc[i].complete_texts\n",
    "        url = df.iloc[i].url\n",
    "        title = df.iloc[i].title\n",
    "        \n",
    "        chunks = text_splitter.split_text(text)\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            f.write(json.dumps({\n",
    "                \"chunk\": chunk,\n",
    "                \"url\": url,\n",
    "                \"title\": title,\n",
    "            })+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunked_text = pd.read_json(\"chunked_text.jsonl\", lines=True)\n",
    "\n",
    "def hf_embed(chunk):\n",
    "    return hf_embeddings.embed_documents([chunk])[0]\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open(\"embedded_chunked_texts.jsonl\", \"a\") as f:\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), unit=\"row\"):\n",
    "        chunk = row['chunk']\n",
    "        url = row['url']\n",
    "        title = row['title']\n",
    "        vector = hf_embed(chunk)\n",
    "        \n",
    "        f.write(json.dumps({\n",
    "            \"chunk\": chunk,\n",
    "            \"url\": url,\n",
    "            \"title\": title,\n",
    "            \"vector\": vector\n",
    "        }) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CUDA Toolkit Documentation 12.5\\nNvidiaSearchI...</td>\n",
       "      <td>https://docs.nvidia.com/cuda/</td>\n",
       "      <td>CUDA Toolkit Documentation 12.5</td>\n",
       "      <td>[-0.030554667115211, -0.047709718346595, -0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CUB The user guide for CUB. CUDA C++ Standard ...</td>\n",
       "      <td>https://docs.nvidia.com/cuda/</td>\n",
       "      <td>CUDA Toolkit Documentation 12.5</td>\n",
       "      <td>[-0.03192675486207, -0.058551348745822004, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cuSPARSE The cuSPARSE library user guide. NPP ...</td>\n",
       "      <td>https://docs.nvidia.com/cuda/</td>\n",
       "      <td>CUDA Toolkit Documentation 12.5</td>\n",
       "      <td>[-0.011653744615614001, -0.008399281650781, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nvFatbin The user guide for the nvFatbin libra...</td>\n",
       "      <td>https://docs.nvidia.com/cuda/</td>\n",
       "      <td>CUDA Toolkit Documentation 12.5</td>\n",
       "      <td>[-0.022603930905461003, -0.041946601122617, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nsight Visual Studio Edition The documentation...</td>\n",
       "      <td>https://docs.nvidia.com/cuda/</td>\n",
       "      <td>CUDA Toolkit Documentation 12.5</td>\n",
       "      <td>[-0.015180315822362001, -0.038234677165746, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               chunk  \\\n",
       "0  CUDA Toolkit Documentation 12.5\\nNvidiaSearchI...   \n",
       "1  CUB The user guide for CUB. CUDA C++ Standard ...   \n",
       "2  cuSPARSE The cuSPARSE library user guide. NPP ...   \n",
       "3  nvFatbin The user guide for the nvFatbin libra...   \n",
       "4  Nsight Visual Studio Edition The documentation...   \n",
       "\n",
       "                             url                            title  \\\n",
       "0  https://docs.nvidia.com/cuda/  CUDA Toolkit Documentation 12.5   \n",
       "1  https://docs.nvidia.com/cuda/  CUDA Toolkit Documentation 12.5   \n",
       "2  https://docs.nvidia.com/cuda/  CUDA Toolkit Documentation 12.5   \n",
       "3  https://docs.nvidia.com/cuda/  CUDA Toolkit Documentation 12.5   \n",
       "4  https://docs.nvidia.com/cuda/  CUDA Toolkit Documentation 12.5   \n",
       "\n",
       "                                              vector  \n",
       "0  [-0.030554667115211, -0.047709718346595, -0.02...  \n",
       "1  [-0.03192675486207, -0.058551348745822004, -0....  \n",
       "2  [-0.011653744615614001, -0.008399281650781, -0...  \n",
       "3  [-0.022603930905461003, -0.041946601122617, -0...  \n",
       "4  [-0.015180315822362001, -0.038234677165746, -0...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "embedded_chunks = pd.read_json(\"embedded_chunked_texts.jsonl\", lines=True)\n",
    "\n",
    "embedded_chunks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(embedded_chunks.vector.to_list(), key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "532055"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(embedded_chunks.chunk.to_list(), key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     90167.000000\n",
       "mean       2096.625506\n",
       "std        4142.869335\n",
       "min           1.000000\n",
       "25%         233.000000\n",
       "50%         870.000000\n",
       "75%        2593.000000\n",
       "85%        3949.000000\n",
       "90%        5301.000000\n",
       "95%        7784.700000\n",
       "max      532055.000000\n",
       "Name: chunk_len, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_chunks[\"chunk_len\"] = embedded_chunks[\"chunk\"].apply(len)\n",
    "\n",
    "embedded_chunks[\"chunk_len\"].describe(percentiles=[0.25, 0.5, 0.75, 0.85, 0.9, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    80190.000000\n",
       "mean      1174.499451\n",
       "std       1233.985462\n",
       "min          1.000000\n",
       "25%        204.000000\n",
       "50%        680.000000\n",
       "75%       1826.000000\n",
       "85%       2668.000000\n",
       "90%       3180.000000\n",
       "95%       3854.550000\n",
       "max       4999.000000\n",
       "Name: chunk_len, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_chunks = embedded_chunks[embedded_chunks[\"chunk_len\"]<5000]\n",
    "embedded_chunks[\"chunk_len\"].describe(percentiles=[0.25, 0.5, 0.75, 0.85, 0.9, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_chunks[\"id\"] = range(len(embedded_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>vector</th>\n",
       "      <th>chunk_len</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CUB The user guide for CUB. CUDA C++ Standard ...</td>\n",
       "      <td>https://docs.nvidia.com/cuda/</td>\n",
       "      <td>CUDA Toolkit Documentation 12.5</td>\n",
       "      <td>[-0.03192675486207, -0.058551348745822004, -0....</td>\n",
       "      <td>496</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cuSPARSE The cuSPARSE library user guide. NPP ...</td>\n",
       "      <td>https://docs.nvidia.com/cuda/</td>\n",
       "      <td>CUDA Toolkit Documentation 12.5</td>\n",
       "      <td>[-0.011653744615614001, -0.008399281650781, -0...</td>\n",
       "      <td>514</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nvFatbin The user guide for the nvFatbin libra...</td>\n",
       "      <td>https://docs.nvidia.com/cuda/</td>\n",
       "      <td>CUDA Toolkit Documentation 12.5</td>\n",
       "      <td>[-0.022603930905461003, -0.041946601122617, -0...</td>\n",
       "      <td>3071</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nsight Visual Studio Edition The documentation...</td>\n",
       "      <td>https://docs.nvidia.com/cuda/</td>\n",
       "      <td>CUDA Toolkit Documentation 12.5</td>\n",
       "      <td>[-0.015180315822362001, -0.038234677165746, -0...</td>\n",
       "      <td>1376</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Compiler SDK \\nlibNVVM API The libNVVM API. l...</td>\n",
       "      <td>https://docs.nvidia.com/cuda/</td>\n",
       "      <td>CUDA Toolkit Documentation 12.5</td>\n",
       "      <td>[-0.024751845747232003, -0.029467459768056002,...</td>\n",
       "      <td>633</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               chunk  \\\n",
       "1  CUB The user guide for CUB. CUDA C++ Standard ...   \n",
       "2  cuSPARSE The cuSPARSE library user guide. NPP ...   \n",
       "3  nvFatbin The user guide for the nvFatbin libra...   \n",
       "4  Nsight Visual Studio Edition The documentation...   \n",
       "5  Compiler SDK \\nlibNVVM API The libNVVM API. l...   \n",
       "\n",
       "                             url                            title  \\\n",
       "1  https://docs.nvidia.com/cuda/  CUDA Toolkit Documentation 12.5   \n",
       "2  https://docs.nvidia.com/cuda/  CUDA Toolkit Documentation 12.5   \n",
       "3  https://docs.nvidia.com/cuda/  CUDA Toolkit Documentation 12.5   \n",
       "4  https://docs.nvidia.com/cuda/  CUDA Toolkit Documentation 12.5   \n",
       "5  https://docs.nvidia.com/cuda/  CUDA Toolkit Documentation 12.5   \n",
       "\n",
       "                                              vector  chunk_len  id  \n",
       "1  [-0.03192675486207, -0.058551348745822004, -0....        496   0  \n",
       "2  [-0.011653744615614001, -0.008399281650781, -0...        514   1  \n",
       "3  [-0.022603930905461003, -0.041946601122617, -0...       3071   2  \n",
       "4  [-0.015180315822362001, -0.038234677165746, -0...       1376   3  \n",
       "5  [-0.024751845747232003, -0.029467459768056002,...        633   4  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_chunks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>vector</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CUB The user guide for CUB. CUDA C++ Standard ...</td>\n",
       "      <td>https://docs.nvidia.com/cuda/</td>\n",
       "      <td>CUDA Toolkit Documentation 12.5</td>\n",
       "      <td>[-0.03192675486207, -0.058551348745822004, -0....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cuSPARSE The cuSPARSE library user guide. NPP ...</td>\n",
       "      <td>https://docs.nvidia.com/cuda/</td>\n",
       "      <td>CUDA Toolkit Documentation 12.5</td>\n",
       "      <td>[-0.011653744615614001, -0.008399281650781, -0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nvFatbin The user guide for the nvFatbin libra...</td>\n",
       "      <td>https://docs.nvidia.com/cuda/</td>\n",
       "      <td>CUDA Toolkit Documentation 12.5</td>\n",
       "      <td>[-0.022603930905461003, -0.041946601122617, -0...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nsight Visual Studio Edition The documentation...</td>\n",
       "      <td>https://docs.nvidia.com/cuda/</td>\n",
       "      <td>CUDA Toolkit Documentation 12.5</td>\n",
       "      <td>[-0.015180315822362001, -0.038234677165746, -0...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Compiler SDK \\nlibNVVM API The libNVVM API. l...</td>\n",
       "      <td>https://docs.nvidia.com/cuda/</td>\n",
       "      <td>CUDA Toolkit Documentation 12.5</td>\n",
       "      <td>[-0.024751845747232003, -0.029467459768056002,...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               chunk  \\\n",
       "1  CUB The user guide for CUB. CUDA C++ Standard ...   \n",
       "2  cuSPARSE The cuSPARSE library user guide. NPP ...   \n",
       "3  nvFatbin The user guide for the nvFatbin libra...   \n",
       "4  Nsight Visual Studio Edition The documentation...   \n",
       "5  Compiler SDK \\nlibNVVM API The libNVVM API. l...   \n",
       "\n",
       "                             url                            title  \\\n",
       "1  https://docs.nvidia.com/cuda/  CUDA Toolkit Documentation 12.5   \n",
       "2  https://docs.nvidia.com/cuda/  CUDA Toolkit Documentation 12.5   \n",
       "3  https://docs.nvidia.com/cuda/  CUDA Toolkit Documentation 12.5   \n",
       "4  https://docs.nvidia.com/cuda/  CUDA Toolkit Documentation 12.5   \n",
       "5  https://docs.nvidia.com/cuda/  CUDA Toolkit Documentation 12.5   \n",
       "\n",
       "                                              vector  id  \n",
       "1  [-0.03192675486207, -0.058551348745822004, -0....   0  \n",
       "2  [-0.011653744615614001, -0.008399281650781, -0...   1  \n",
       "3  [-0.022603930905461003, -0.041946601122617, -0...   2  \n",
       "4  [-0.015180315822362001, -0.038234677165746, -0...   3  \n",
       "5  [-0.024751845747232003, -0.029467459768056002,...   4  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del embedded_chunks[\"chunk_len\"]\n",
    "\n",
    "embedded_chunks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_chunks[\"title\"] = embedded_chunks[\"title\"].astype(str)\n",
    "embedded_chunks[\"url\"] = embedded_chunks[\"url\"].astype(str)\n",
    "embedded_chunks[\"chunk\"] = embedded_chunks[\"chunk\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient, DataType\n",
    "import os\n",
    "\n",
    "CLUSTER_ENDPOINT = \"https://in03-6f08bdb85fec6ea.api.gcp-us-west1.zillizcloud.com\"\n",
    "TOKEN = os.getenv('ZILLIZ_TOKEN')\n",
    "\n",
    "# 1. Set up a Milvus client\n",
    "client = MilvusClient(\n",
    "    uri=CLUSTER_ENDPOINT,\n",
    "    token=TOKEN \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = MilvusClient.create_schema(\n",
    "    auto_id=False,\n",
    "    enable_dynamic_field=True,\n",
    ")\n",
    "\n",
    "schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)\n",
    "schema.add_field(field_name=\"vector\", datatype=DataType.FLOAT_VECTOR, dim=768)\n",
    "schema.add_field(field_name=\"chunk\", datatype=DataType.VARCHAR, max_length=8000)\n",
    "schema.add_field(field_name=\"url\", datatype=DataType.VARCHAR, max_length=1000)\n",
    "schema.add_field(field_name=\"title\", datatype=DataType.VARCHAR, max_length=1000)\n",
    "\n",
    "\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"cuda_data\", \n",
    "    schema=schema, \n",
    ")\n",
    "\n",
    "index_params = MilvusClient.prepare_index_params()\n",
    "\n",
    "index_params.add_index(\n",
    "    field_name=\"vector\",\n",
    "    metric_type=\"COSINE\",\n",
    "    index_type=\"HNSW\",\n",
    "    index_name=\"vector_index\",\n",
    "    params={ \"nlist\": 128 }\n",
    ")\n",
    "\n",
    "client.create_index(\n",
    "    collection_name=\"cuda_data\",\n",
    "    index_params=index_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vector_index']\n"
     ]
    }
   ],
   "source": [
    "res = client.list_indexes(\n",
    "    collection_name=\"cuda_data\"\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nlist': '128',\n",
       " 'index_type': 'HNSW',\n",
       " 'metric_type': 'COSINE',\n",
       " 'field_name': 'vector',\n",
       " 'index_name': 'vector_index'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = client.describe_index(\n",
    "    collection_name=\"cuda_data\",\n",
    "    index_name=\"vector_index\"\n",
    ")\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = embedded_chunks.to_json(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80190"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parts = [data[i * len(data) // 10: (i + 1) * len(data) // 10] for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8019, 8019, 8019, 8019, 8019, 8019, 8019, 8019, 8019, 8019]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(i) for i in data_parts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for n,i in enumerate(data_parts):\n",
    "    res = client.insert(\n",
    "        collection_name=\"cuda_data\",\n",
    "        data=i\n",
    "    )\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings()\n",
    "question = \"What is the CUDA version suppoorted by NVIDIA RTX\"\n",
    "embedded_query = hf_embeddings.embed_query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.007597993593662977,\n",
       " -0.0635538399219513,\n",
       " -0.024716200307011604,\n",
       " 0.04655836150050163,\n",
       " -0.053499311208724976,\n",
       " -0.010638721287250519,\n",
       " 0.0255771242082119,\n",
       " 0.03909706696867943,\n",
       " -0.02049882523715496,\n",
       " -0.0034186614211648703]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_query[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.load_collection(\n",
    "    collection_name=\"cuda_data\",\n",
    ")\n",
    "\n",
    "res = client.search(\n",
    "    collection_name=\"cuda_data\", \n",
    "    data=[embedded_query],\n",
    "    limit=10, \n",
    "    search_params={\"metric_type\": \"COSINE\", \"params\": {}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[70922, 71366, 70584, 26023, 58052, 26620, 72425, 39868, 73, 25906]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = [i[\"id\"] for i in res[0]]\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = client.get(\n",
    "    collection_name=\"cuda_data\",\n",
    "    ids=ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['https://docs.nvidia.com/cuda/cuda-features-archive/index.html',\n",
       "  'CUDA Features Archive\\n1. CUDA 11.6 Features\\n1.1. Compiler\\n1.1.1. VS2022 Support\\n1.1.2. New instructions in public PTX\\n1.1.3. Unused Kernel Optimization\\n1.1.4. New -arch=native option\\n1.1.5. Generate PTX from nvlink:\\n1.1.6. Bullseye support\\n1.1.7. INT128 developer tool support\\n2. Notices\\n2.1.'],\n",
       " ['https://docs.nvidia.com/vgpu/10.0/grid-vgpu-release-notes-vmware-vsphere/index.html',\n",
       "  \"x versions\\nSince 10.2: 7.0, 6.7, 6.5 10.0, 10.1 only: 6.7, 6.5\\nSince 10.2: 7.0, 6.7, 6.5 10.0, 10.1 only: 6.7, 6.5\\nUbuntu 18.04 LTS\\nSince 10.2: 7.0, 6.7, 6.5 10.0, 10.1 only: 6.7, 6.5\\nSince 10.2: 7.0, 6.7, 6.5 10.0, 10.1 only: 6.7, 6.5\\nUbuntu 16.04 LTS\\nSince 10.2: 7.0, 6.7, 6.5 10.0, 10.1 only: 6.7, 6.5\\nSince 10.2: 7.0, 6.7, 6.5 10.0, 10.1 only: 6.7, 6.5\\nUbuntu 14.04 LTS\\nSince 10.2: 7.0, 6.7, 6.5 10.0, 10.1 only: 6.7, 6.5\\nSince 10.2: 7.0, 6.7, 6.5 10.0, 10.1 only: 6.7, 6.5\\nSUSE Linux Enterprise Server 12 SP3\\nSince 10.2: 7.0, 6.7, 6.5 10.0, 10.1 only: 6.7, 6.5\\nSince 10.2: 7.0, 6.7, 6.5 10.0, 10.1 only: 6.7, 6.5\\n2.4. NVIDIA CUDA Toolkit Version Support\\nThe releases in this release family of NVIDIA vGPU software support NVIDIA CUDA Toolkit 10.2. For more information about NVIDIA CUDA Toolkit, see CUDA Toolkit 10.2 Documentation . Note:\\nIf you are using NVIDIA vGPU software with CUDA on Linux, avoid conflicting installation methods by installing CUDA from a distribution-independent runfile package. Do not install CUDA from distribution-specific RPM or Deb package. To ensure that the NVIDIA vGPU software graphics driver is not overwritten when CUDA is installed, deselect the CUDA driver when selecting the CUDA components to install. For more information, see NVIDIA CUDA Installation Guide for Linux . 2.5. vGPU Migration Support\\nvGPU migration, which includes vMotion and suspend-resume, is supported only on a subset of supported GPUs, VMware vSphere Hypervisor (ESXi) releases, and guest operating systems. Supported GPUs:\\nTesla M6\\nTesla M10\\nTesla M60\\nTesla P4\\nTesla P6\\nTesla P40\\nTesla V100 SXM2\\nTesla V100 SXM2 32GB\\nTesla V100 PCIe\\nTesla V100 PCIe 32GB\\nTesla V100S PCIe 32GB\\nTesla V100 FHHL\\nTesla T4\\nQuadro RTX 6000\\nQuadro RTX 6000 passive\\nQuadro RTX 8000\\nQuadro RTX 8000 passive\\nSupported VMware vSphere Hypervisor (ESXi) releases:\\nSince 10.2: Release 7.0 and compatible updates support vMotion with vGPU and suspend-resume with vGPU. Release 6.7 U1 and compatible updates support vMotion with vGPU and suspend-resume with vGPU. Release 6.7 supports only suspend-resume with vGPU. Releases earlier than 6.7 do not support any form of vGPU migration. Supported guest OS releases: Windows and Linux. Migration between hosts that are running different releases of the vGPU manager in this major release branch are supported except for migrations between hosts running certain combinations of vGPU manager releases with certain GPUs. See the following table for details. GPU\\nUnsupported Migrations\\nTesla V100S PCIe 32GB\\nBetween hosts running 10.0 and hosts running 10.1 through 10.4\\nTesla T4\\nBetween hosts running 10.0 or 10.1 and hosts running 10.2 through 10.4\\n2.6. Multiple vGPU Support\\nTo support applications and workloads that are compute or graphics intensive, multiple vGPUs can be added to a single VM. The assignment of more than one vGPU to a VM is supported only on a subset of vGPUs and VMware vSphere Hypervisor (ESXi) releases. Supported vGPUs\\nOnly Q-series and C-series vGPUs that are allocated all of the physical GPU's frame buffer are supported. GPU Architecture\\nBoard\\nvGPU\\nTuring\\nTesla T4\\nT4-16Q\\nT4-16C\\nQuadro RTX 6000\\nRTX6000-24Q\\nRTX6000-24C\\nQuadro RTX 6000 passive\\nRTX6000P-24Q\\nRTX6000P-24C\\nQuadro RTX 8000\\nRTX8000-48Q\\nRTX8000-48C\\nQuadro RTX 8000 passive\\nRTX8000P-48Q\\nRTX8000P-48C\\nVolta\\nTesla V100 SXM2 32GB\\nV100DX-32Q\\nV100D-32C\\nTesla V100 PCIe 32GB\\nV100D-32Q\\nV100D-32C\\nTesla V100S PCIe 32GB\\nV100S-32Q\\nV100S-32C\\nTesla V100 SXM2\\nV100X-16Q\\nV100X-16C\\nTesla V100 PCIe\\nV100-16Q\\nV100-16C\\nTesla V100 FHHL\\nV100L-16Q\\nV100L-16C\\nPascal\\nTesla P100 SXM2\\nP100X-16Q\\nP100X-16C\\nTesla P100 PCIe 16GB\\nP100-16Q\\nP100-16C\\nTesla P100 PCIe 12GB\\nP100C-12Q\\nP100C-12C\\nTesla P40\\nP40-24Q\\nP40-24C\\nTesla P6\\nP6-16Q\\nP6-16C\\nTesla P4\\nP4-8Q\\nP4-8C\\nMaxwell\\nTesla M60\\nM60-8Q\\nTesla M10\\nM10-8Q\\nTesla M6\\nM6-8Q\\nMaximum vGPUs per VM\\nNVIDIA vGPU software supports up to a maximum of four vGPUs per VM on VMware vSphere Hypervisor (ESXi). Supported Hypervisor Releases\\nOnly these VMware vSphere Hypervisor (ESXi) releases:\\nSince 10.2: 7.0 and 6.7 U3 and later compatible updates\\n10.0, 10.1 only: 6.7 U3 and later compatible updates\\nIf you upgraded to VMware vSphere 6.7 Update 3 from an earlier version and are using VMs that were created with that version, change the VM compatibility to vSphere 6.7 Update 2 and later .\"],\n",
       " ['https://docs.nvidia.com/vgpu/10.0/grid-vgpu-release-notes-red-hat-el-kvm/index.html',\n",
       "  \"NVIDIA CUDA Toolkit Version Support\\nThe releases in this release family of NVIDIA vGPU software support NVIDIA CUDA Toolkit 10.2. For more information about NVIDIA CUDA Toolkit, see CUDA Toolkit 10.2 Documentation . Note:\\nIf you are using NVIDIA vGPU software with CUDA on Linux, avoid conflicting installation methods by installing CUDA from a distribution-independent runfile package. Do not install CUDA from distribution-specific RPM or Deb package. To ensure that the NVIDIA vGPU software graphics driver is not overwritten when CUDA is installed, deselect the CUDA driver when selecting the CUDA components to install. For more information, see NVIDIA CUDA Installation Guide for Linux . 2.5. Multiple vGPU Support\\nTo support applications and workloads that are compute or graphics intensive, multiple vGPUs can be added to a single VM. The assignment of more than one vGPU to a VM is supported only on a subset of vGPUs and Red Hat Enterprise Linux with KVM releases. Supported vGPUs\\nOnly Q-series and C-series vGPUs that are allocated all of the physical GPU's frame buffer are supported. GPU Architecture\\nBoard\\nvGPU\\nTuring\\nTesla T4\\nT4-16Q\\nT4-16C\\nQuadro RTX 6000\\nRTX6000-24Q\\nRTX6000-24C\\nQuadro RTX 6000 passive\\nRTX6000P-24Q\\nRTX6000P-24C\\nQuadro RTX 8000\\nRTX8000-48Q\\nRTX8000-48C\\nQuadro RTX 8000 passive\\nRTX8000P-48Q\\nRTX8000P-48C\\nVolta\\nTesla V100 SXM2 32GB\\nV100DX-32Q\\nV100D-32C\\nTesla V100 PCIe 32GB\\nV100D-32Q\\nV100D-32C\\nTesla V100S PCIe 32GB\\nV100S-32Q\\nV100S-32C\\nTesla V100 SXM2\\nV100X-16Q\\nV100X-16C\\nTesla V100 PCIe\\nV100-16Q\\nV100-16C\\nTesla V100 FHHL\\nV100L-16Q\\nV100L-16C\\nPascal\\nTesla P100 SXM2\\nP100X-16Q\\nP100X-16C\\nTesla P100 PCIe 16GB\\nP100-16Q\\nP100-16C\\nTesla P100 PCIe 12GB\\nP100C-12Q\\nP100C-12C\\nTesla P40\\nP40-24Q\\nP40-24C\\nTesla P6\\nP6-16Q\\nP6-16C\\nTesla P4\\nP4-8Q\\nP4-8C\\nMaxwell\\nTesla M60\\nM60-8Q\\nTesla M10\\nM10-8Q\\nTesla M6\\nM6-8Q\\nMaximum vGPUs per VM\\nNVIDIA vGPU software supports up to a maximum of 16 vGPUs per VM on Red Hat Enterprise Linux with KVM. Supported Hypervisor Releases\\nOnly these Red Hat Enterprise Linux with KVM releases:\\nSince 10.2: 8.2, 8.1, 7.8, 7.7, and 7.6\\n10.1 only: 8.1, 7.7, 7.6, and 7.5\\n10.0 only: 8.1, 8.0, 7.7, 7.6, and 7.5\\nRHV 4.3 and 4.2 only. 2.6. Peer-to-Peer CUDA Transfers over NVLink Support\\nPeer-to-peer CUDA transfers enable device memory between vGPUs on different GPUs that are assigned to the same VM to be accessed from within the CUDA kernels. NVLink is a high-bandwidth interconnect that enables fast communication between such vGPUs. Peer-to-Peer CUDA Transfers over NVLink is supported only on a subset of vGPUs, Red Hat Enterprise Linux with KVM releases, and guest OS releases. Supported vGPUs\\nOnly Q-series and C-series vGPUs that are allocated all of the physical GPU's frame buffer on physical GPUs that support NVLink are supported. GPU Architecture\\nBoard\\nvGPU\\nTuring\\nQuadro RTX 6000\\nRTX6000-24Q\\nRTX6000-24C\\nQuadro RTX 6000 passive\\nRTX6000P-24Q\\nRTX6000P-24C\\nQuadro RTX 8000\\nRTX8000-48Q\\nRTX8000-48C\\nQuadro RTX 8000 passive\\nRTX8000P-48Q\\nRTX8000P-48C\\nVolta\\nTesla V100 SXM2 32GB\\nV100DX-32Q\\nV100DX-32C\\nTesla V100 SXM2\\nV100X-16Q\\nV100X-16C\\nPascal\\nTesla P100 SXM2\\nP100X-16Q\\nP100X-16C\\nSupported Hypervisor Releases\\nPeer-to-Peer CUDA Transfers over NVLink are supported on all hypervisor releases that support the assignment of more than one vGPU to a VM. For details, see Multiple vGPU Support . Supported Guest OS Releases\\nLinux only. Peer-to-Peer CUDA Transfers over NVLink are not supported on Windows. Limitations\\nOnly direct connections are supported.\"],\n",
       " ['https://docs.nvidia.com/vgpu/9.0/grid-vgpu-release-notes-red-hat-el-kvm/index.html',\n",
       "  \"NVIDIA CUDA Toolkit Version Support\\nThe releases in this release family of NVIDIA vGPU software support NVIDIA CUDA Toolkit 10.1 Update 1. For more information about NVIDIA CUDA Toolkit, see CUDA Toolkit 10.1 Documentation . Note:\\nIf you are using NVIDIA vGPU software with CUDA on Linux, avoid conflicting installation methods by installing CUDA from a distribution-independent runfile package. Do not install CUDA from distribution-specific RPM or Deb package. To ensure that the NVIDIA vGPU software graphics driver is not overwritten when CUDA is installed, deselect the CUDA driver when selecting the CUDA components to install. For more information, see NVIDIA CUDA Installation Guide for Linux . 2.5. Multiple vGPU Support\\nTo support applications and workloads that are compute or graphics intensive, multiple vGPUs can be added to a single VM. The assignment of more than one vGPU to a VM is supported only on a subset of vGPUs and Red Hat Enterprise Linux with KVM releases. Supported vGPUs\\nOnly Q-series and C-series vGPUs that are allocated all of the physical GPU's frame buffer are supported. GPU Architecture\\nBoard\\nvGPU\\nTuring\\nTesla T4\\nT4-16Q\\nT4-16C\\nQuadro RTX 6000\\nRTX6000-24Q\\nQuadro RTX 8000\\nRTX8000-48Q\\nVolta\\nTesla V100 SXM2 32GB\\nV100DX-32Q\\nV100D-32C\\nTesla V100 PCIe 32GB\\nV100D-32Q\\nV100D-32C\\nTesla V100 SXM2\\nV100X-16Q\\nV100X-16C\\nTesla V100 PCIe\\nV100-16Q\\nV100-16C\\nTesla V100 FHHL\\nV100L-16Q\\nV100L-16C\\nPascal\\nTesla P100 SXM2\\nP100X-16Q\\nP100X-16C\\nTesla P100 PCIe 16GB\\nP100-16Q\\nP100-16C\\nTesla P100 PCIe 12GB\\nP100C-12Q\\nP100C-12C\\nTesla P40\\nP40-24Q\\nP40-24C\\nTesla P6\\nP6-16Q\\nP6-16C\\nTesla P4\\nP4-8Q\\nP4-8C\\nMaxwell\\nTesla M60\\nM60-8Q\\nTesla M10\\nM10-8Q\\nTesla M6\\nM6-8Q\\nMaximum vGPUs per VM\\nNVIDIA vGPU software supports up to a maximum of 16 vGPUs per VM on Red Hat Enterprise Linux with KVM. Supported Hypervisor Releases\\nRed Hat Enterprise Linux with KVM 7.6 and 7.5 only.\"],\n",
       " ['https://docs.nvidia.com/cuda/archive/12.4.0/cuda-features-archive/index.html',\n",
       "  'CUDA Features Archive\\n1. CUDA 11.6 Features\\n1.1. Compiler\\n1.1.1. VS2022 Support\\n1.1.2. New instructions in public PTX\\n1.1.3. Unused Kernel Optimization\\n1.1.4. New -arch=native option\\n1.1.5. Generate PTX from nvlink:\\n1.1.6. Bullseye support\\n1.1.7. INT128 developer tool support\\n2. Notices\\n2.1.'],\n",
       " ['https://docs.nvidia.com/vgpu/13.0/product-support-matrix/index.html',\n",
       "  'NVIDIA CUDA Toolkit version supported: 11.4\\nSee NVIDIA CUDA Toolkit and OpenCL Support on NVIDIA vGPU Software in Virtual GPU Software User Guide for details about supported features and limitations. For a list of validated server platforms, refer to NVIDIA Virtual GPU Certified Servers . Citrix Hypervisor Support\\nLinux with KVM Support\\nMicrosoft Windows Server Support\\nNutanix AHV Support\\nRed Hat Enterprise Linux with KVM Support\\nSince 13.1: Ubuntu Support\\nVMware vSphere ESXi Support\\nCitrix Hypervisor Support\\nDriver Package\\nHypervisor or Bare-Metal OS\\nSoftware Product Deployment\\nHardware Supported\\nGuest OS Support 1 , 2\\nSupported Virtualization Products 3 , 4\\nNVIDIA vGPU for XenServer 8.2 5\\nCitrix Hypervisor 8.2\\nNVIDIA vGPU\\nGPU pass through\\nA10\\nA16\\nA30 7\\nA40 6 , A100 7\\nM6, M10, M60\\nP4, P6, P40, P100, P100 12GB\\nT4\\nV100\\nRTX A5000 6\\nRTX A6000 6\\nRTX 6000, RTX 6000 passive, RTX 8000, RTX 8000 passive 6\\nSince 13.7: Windows 10 22H2 8\\n13.1-13.6 only: Windows 10 21H2 8\\n13.0 only: Windows 10 21H1 8\\nCitrix Virtual Apps and Desktops\\nHP Anywhere 9\\n13.0-13.8 only: Windows Server 2012 R2 10\\nWindows Server 2016 1607, 1709\\nWindows Server 2019\\nWindows Server 2022\\nCitrix Virtual Apps and Desktops\\nRDSH\\nHP Anywhere 9\\nSince 13.11: Red Hat Enterprise Linux 8.8, 8.10\\n13.10 only: Red Hat Enterprise Linux 8.6, 8.8, 8.9\\n13.8, 13.9 only: Red Hat Enterprise Linux 8.6, 8.8\\n13.5-13.7 only: Red Hat Enterprise Linux 8.4, 8.6, 8.7\\n13.3, 13.4 only: Red Hat Enterprise Linux 8.4, 8.6\\n13.2 only: Red Hat Enterprise Linux 8.2, 8.4, 8.5\\n13.0, 13,1 only: Red Hat Enterprise Linux 8.1, 8.2, 8.4\\n13.2-13.11 only: Red Hat Enterprise Linux 7.9\\n13.0, 13,1 only: Red Hat Enterprise Linux 7.7-7.9\\nUbuntu 14.04 LTS/16.04/18.04 LTS/20.04 LTS\\nVNC\\nHP Anywhere 9\\n13.0-13.3 only: NVIDIA vGPU for XenServer 7.1 5\\nCitrix XenServer 7.1\\nNVIDIA vGPU\\nGPU pass through\\nM6, M10, M60\\nP4, P6, P40, P100\\nP100 12GB\\nV100\\nRTX 6000, RTX 8000 6\\n13.1-13.3 only: Windows 10 21H2 8\\n13.0 only: Windows 10 21H1 8\\nCitrix Virtual Apps and Desktops\\nHP Anywhere 9\\nWindows Server 2012 R2 10\\nWindows Server 2016 1607, 1709\\nWindows Server 2019 11\\nWindows Server 2022 12 , 13\\nCitrix Virtual Apps and Desktops\\nRDSH\\nHP Anywhere 9\\n13.2-13.11 only: Red Hat Enterprise Linux 7.9\\n13.0, 13,1 only: Red Hat Enterprise Linux 7.7-7.9\\nUbuntu 14.04 LTS/16.04/18.04 14 LTS\\nVNC\\nHP Anywhere 9\\nLinux with KVM Support\\nNVIDIA vGPU software is supported on Linux with KVM platforms only by specific hypervisor software vendors. For information about which NVIDIA vGPU software releases and hypervisor software releases are supported, consult the documentation from your hypervisor vendor.'],\n",
       " ['https://docs.nvidia.com/vgpu/14.0/product-support-matrix/index.html',\n",
       "  'NVIDIA CUDA Toolkit version supported: 11.6\\nSee NVIDIA CUDA Toolkit and OpenCL Support on NVIDIA vGPU Software in Virtual GPU Software User Guide for details about supported features and limitations. For a list of validated server platforms, refer to NVIDIA GRID Certified Servers . Citrix Hypervisor Support\\nLinux with KVM Support\\nMicrosoft Windows Server Support\\nNutanix AHV Support\\nRed Hat Enterprise Linux with KVM Support\\nUbuntu Support\\nVMware vSphere ESXi Support\\nCitrix Hypervisor Support\\nDriver Package\\nHypervisor or Bare-Metal OS\\nSoftware Product Deployment\\nHardware Supported\\nGuest OS Support 1 , 2\\nSupported Virtualization Products 3 , 4\\nNVIDIA vGPU for XenServer 8.2 5\\nCitrix Hypervisor 8.2\\nNVIDIA vGPU\\nGPU pass through\\nA2\\nA10\\nA16\\nA30 7 , A30X 7\\nA40 6 , A100 7 , A100X 7\\nM6, M10, M60\\nP4, P6, P40, P100, P100 12GB\\nT4\\nV100\\nRTX A5000 6\\nRTX A5500 6\\nRTX A6000 6\\nRTX 6000, RTX 6000 passive, RTX 8000, RTX 8000 passive 6\\nWindows 10 21H2 8\\nCitrix Virtual Apps and Desktops\\nTeradici Cloud Access Software\\n14.0, 14.1 only: Windows Server 2016 1607, 1709\\nWindows Server 2019\\nWindows Server 2022\\nCitrix Virtual Apps and Desktops\\nRDSH\\nTeradici Cloud Access Software\\nSince 14.3: Red Hat Enterprise Linux 8.4, 8.6, 8.7\\n14.1, 14.2 only: Red Hat Enterprise Linux 8.4, 8.6\\n14.0 only: Red Hat Enterprise Linux 8.2, 8.4, 8.5\\nRed Hat Enterprise Linux 7.9\\nUbuntu 20.04 LTS\\nUbuntu 18.04 LTS\\nUbuntu 16.04 LTS\\nUbuntu 14.04 LTS\\nVNC\\nTeradici Cloud Access Software\\n14.0, 14.1 only: NVIDIA vGPU for XenServer 7.1 5\\nCitrix XenServer 7.1\\nNVIDIA vGPU\\nGPU pass through\\nM6, M10, M60\\nP4, P6, P40, P100\\nP100 12GB\\nV100\\nRTX 6000, RTX 8000 6\\nWindows 10 21H2 8\\nCitrix Virtual Apps and Desktops\\nTeradici Cloud Access Software\\n14.0, 14.1 only: Windows Server 2016 1607, 1709\\nWindows Server 2019 9\\nWindows Server 2022 10 , 11\\nCitrix Virtual Apps and Desktops\\nRDSH\\nTeradici Cloud Access Software\\nRed Hat Enterprise Linux 7.9\\nUbuntu 18.04 LTS 12\\nUbuntu 16.04 LTS\\nUbuntu 14.04 LTS\\nVNC\\nTeradici Cloud Access Software\\nLinux with KVM Support\\nNVIDIA vGPU software is supported on Linux with KVM platforms only by specific hypervisor software vendors. For information about which NVIDIA vGPU software releases and hypervisor software releases are supported, consult the documentation from your hypervisor vendor.'],\n",
       " ['https://docs.nvidia.com/vgpu/15.0/product-support-matrix/index.html',\n",
       "  'NVIDIA CUDA Toolkit version supported: 12.0\\nSee NVIDIA CUDA Toolkit and OpenCL Support on NVIDIA vGPU Software in Virtual GPU Software User Guide for details about supported features and limitations. For a list of validated server platforms, refer to NVIDIA GRID Certified Servers . Citrix Hypervisor Support\\nLinux with KVM Support\\nMicrosoft Azure Stack HCI Support\\nMicrosoft Windows Server Support\\nNutanix AHV Support\\nRed Hat Enterprise Linux with KVM Support\\nUbuntu Support\\nVMware vSphere ESXi Support\\nCitrix Hypervisor Support\\nDriver Package\\nHypervisor or Bare-Metal OS\\nSoftware Product Deployment\\nHardware Supported\\nGuest OS Support 1 , 2 , 3\\nSupported Virtualization Products 4 , 5\\nNVIDIA vGPU for XenServer 8.2 6\\nCitrix Hypervisor 8.2\\nNVIDIA vGPU\\nGPU pass through\\nSince 15.2: L4\\nSince 15.1: L40\\nSince 15.1: RTX 6000 Ada\\nA2\\nA10\\nA16\\nA30 8 , A30X 8\\nA40 7 , A100 8 , A100X 8\\nA800 8\\nH100 8\\nSince 15.2: H800 8\\nM6, M10, M60\\nP4, P6, P40, P100, P100 12GB\\nT4\\nV100\\nRTX A5000 7\\nRTX A5500 7\\nRTX A6000 7\\nRTX 6000, RTX 6000 passive, RTX 8000, RTX 8000 passive 7\\nWindows 10 22H2 10\\nCitrix Virtual Apps and Desktops\\nHP Anywhere 11\\nWindows Server 2019\\nWindows Server 2022\\nCitrix Virtual Apps and Desktops\\nRDSH\\nHP Anywhere 11\\nSince 15.3: Red Hat Enterprise Linux 8.6, 8.8\\n15.0-15.2 only: Red Hat Enterprise Linux 8.4, 8.6, 8.7\\nRed Hat Enterprise Linux 7.9\\nUbuntu 20.04 LTS\\nUbuntu 18.04 LTS\\nVNC\\nHP Anywhere 11\\nLinux with KVM Support\\nNVIDIA vGPU software is supported on Linux with KVM platforms only by specific hypervisor software vendors. For information about which NVIDIA vGPU software releases and hypervisor software releases are supported, consult the documentation from your hypervisor vendor.'],\n",
       " ['https://docs.nvidia.com/vgpu/12.0/product-support-matrix/index.html',\n",
       "  'NVIDIA CUDA Toolkit version supported: 11.2\\nSee NVIDIA CUDA Toolkit and OpenCL Support on NVIDIA vGPU Software in Virtual GPU Software User Guide for details about supported features and limitations. For a list of validated server platforms, refer to NVIDIA GRID Certified Servers . Citrix Hypervisor Support\\nMicrosoft Windows Server Support\\nLinux with KVM Support\\nNutanix AHV Support\\nRed Hat Enterprise Linux with KVM Support\\nVMware vSphere ESXi Support\\nCitrix Hypervisor Support\\nDriver Package\\nHypervisor or Bare-Metal OS\\nSoftware Product Deployment\\nHardware Supported\\nGuest OS Support 1 , 2\\nSupported Virtualization Products 3 , 4\\nNVIDIA vGPU for XenServer 8.2 5\\nCitrix Hypervisor 8.2\\nNVIDIA vGPU\\nGPU pass through\\nSince 12.2: A10\\nA40 6 , A100 7\\nM6, M10, M60\\nP4, P6, P40, P100, P100 12GB\\nT4\\nV100\\nSince 12.2: RTX A5000 6\\nRTX A6000 6\\nRTX 6000, RTX 6000 passive, RTX 8000, RTX 8000 passive 6\\nWindows 10 21H1 8\\nCitrix Virtual Apps and Desktops\\nTeradici Cloud Access Software\\nWindows Server 2012 R2 9\\nWindows Server 2016 1607, 1709\\nWindows Server 2019\\nCitrix Virtual Apps and Desktops\\nRDSH\\nTeradici Cloud Access Software\\nSince 12.3: Red Hat Enterprise Linux 8.1, 8.2, 8.4\\n12.2 only: Red Hat Enterprise Linux 8.1-8.4\\n12.0, 12.1 only: Red Hat Enterprise Linux 8.1-8.3\\nRed Hat Enterprise Linux 7.7-7.9\\nUbuntu 14.04 LTS/16.04/18.04 LTS/20.04 LTS\\nVNC\\nTeradici Cloud Access Software\\nNVIDIA vGPU for XenServer 7.1 5\\nCitrix XenServer 7.1\\nNVIDIA vGPU\\nGPU pass through\\nM6, M10, M60\\nP4, P6, P40, P100\\nP100 12GB\\nV100\\nRTX 6000, RTX 8000 6\\nWindows 10 21H1 8\\nCitrix Virtual Apps and Desktops\\nTeradici Cloud Access Software\\nWindows Server 2012 R2 9\\nWindows Server 2016 1607, 1709\\nWindows Server 2019 10\\nCitrix Virtual Apps and Desktops\\nRDSH\\nTeradici Cloud Access Software\\nRed Hat Enterprise Linux 7.7-7.9\\nUbuntu 14.04 LTS/16.04/18.04 11 LTS\\nVNC\\nTeradici Cloud Access Software\\n12.0-12.2 only: NVIDIA vGPU for XenServer 7.0 5\\nCitrix XenServer 7.0\\nNVIDIA vGPU\\nGPU pass through\\nM6, M10, M60\\nP4, P6, P40, P100,\\nP100 12GB\\nV100\\nRTX 6000, RTX 8000 6\\nWindows 10 21H1 8\\nCitrix Virtual Apps and Desktops\\nTeradici Cloud Access Software\\nWindows Server 2012 R2 9\\nWindows Server 2016 1607, 1709\\nCitrix Virtual Apps and Desktops\\nRDSH\\nTeradici Cloud Access Software\\nRed Hat Enterprise Linux 7.7-7.9\\nUbuntu 14.04 LTS/16.04/18.04 LTS\\nVNC\\nTeradici Cloud Access Software\\nLinux with KVM Support\\nNVIDIA vGPU software is supported on Linux with KVM platforms only by specific hypervisor software vendors. For information about which NVIDIA vGPU software releases and hypervisor software releases are supported, consult the documentation from your hypervisor vendor.'],\n",
       " ['https://docs.nvidia.com/vgpu/11.0/product-support-matrix/index.html',\n",
       "  'NVIDIA CUDA Toolkit version supported: 11.0\\nSee NVIDIA CUDA Toolkit and OpenCL Support on NVIDIA vGPU Software in Virtual GPU Software User Guide for details about supported features and limitations. For a list of validated server platforms, refer to NVIDIA GRID Certified Servers . Citrix Hypervisor Support\\nLinux with KVM Support\\nMicrosoft Windows Server Support\\nNutanix AHV Support\\nRed Hat Enterprise Linux with KVM Support\\nVMware vSphere ESXi Support\\nCitrix Hypervisor Support\\nDriver Package\\nHypervisor or Bare-Metal OS\\nSoftware Product Deployment\\nHardware Supported\\nGuest OS Support 1 , 2\\nSupported Virtualization Products 3 , 4\\nNVIDIA vGPU for XenServer 8.2 5\\nCitrix Hypervisor 8.2\\nNVIDIA vGPU\\nPass-through GPU\\nA100 6\\nM6, M10, M60\\nP4, P6, P40, P100, P100 12GB\\nT4\\nV100\\nRTX 6000, RTX 6000 passive, RTX 8000, RTX 8000 passive 7\\nSince 11.12: Windows 10 22H2 8\\n11.3-11.11 only: Windows 10 21H1 8\\n11.0-11.2 only: Windows 10 2004 8\\nCitrix Virtual Apps and Desktops\\nHP Anywhere 9\\nWindows Server 2012 R2 10\\nWindows Server 2016 1607, 1709\\nWindows Server 2019\\nCitrix Virtual Apps and Desktops\\nRDSH\\nHP Anywhere 9\\nSince 11.13: Red Hat Enterprise Linux 8.6, 8.8\\n11.10-11.12 only: Red Hat Enterprise Linux 8.4, 8.6, 8.7\\n11.8, 11.9 only: Red Hat Enterprise Linux 8.4, 8.6\\n11.7 only: Red Hat Enterprise Linux 8.2, 8.4, 8.5\\n11.5-11.6 only: Red Hat Enterprise Linux 8.1, 8.2, 8.4\\n11.4 only: Red Hat Enterprise Linux 8.1-8.4\\n11.3 only: Red Hat Enterprise Linux 8.1-8.3\\n11.0-11.2 only: Red Hat Enterprise Linux 8.1, 8.2\\nSince 11.7: Red Hat Enterprise Linux 7.9\\n11.3-11.6 only: Red Hat Enterprise Linux 7.7-7.9\\n11.1-11.2 only: Red Hat Enterprise Linux 7.6-7.9\\n11.0 only: Red Hat Enterprise Linux 7.6-7.8\\nUbuntu 14.04 LTS/16.04/18.04 LTS/20.04 LTS\\nVNC\\nHP Anywhere 9\\n11.0-11.2 only: NVIDIA vGPU for XenServer 8.1 5\\nCitrix Hypervisor 8.1\\nNVIDIA vGPU\\nPass-through GPU\\nA100 6\\nM6, M10, M60\\nP4, P6, P40, P100, P100 12GB\\nT4\\nV100\\nRTX 6000, RTX 6000 passive, RTX 8000, RTX 8000 passive 7\\nWindows 10 2004 8\\nCitrix Virtual Apps and Desktops\\nHP Anywhere 9\\nWindows Server 2012 R2 10\\nWindows Server 2016 1607, 1709\\nWindows Server 2019\\nCitrix Virtual Apps and Desktops\\nRDSH\\nHP Anywhere 9\\nRed Hat Enterprise Linux 8.1, 8.2\\n11.1, 11.2 only: Red Hat Enterprise Linux 7.6-7.9\\n11.0 only: Red Hat Enterprise Linux 7.6-7.8\\nUbuntu 14.04 LTS/16.04/18.04 LTS\\nVNC\\nHP Anywhere 9\\n11.0-11.8 only: NVIDIA vGPU for XenServer 7.1 5\\nCitrix XenServer 7.1\\nNVIDIA vGPU\\nPass-through GPU\\nM6, M10, M60\\nP4, P6, P40, P100\\nP100 12GB\\nV100\\nRTX 6000, RTX 8000 7\\n11.3-11.8 only: Windows 10 21H1 8\\n11.0-11.2 only: Windows 10 2004 8\\nCitrix Virtual Apps and Desktops\\nHP Anywhere 9\\nWindows Server 2012 R2 10\\nWindows Server 2016 1607, 1709\\nWindows Server 2019 11\\nCitrix Virtual Apps and Desktops\\nRDSH\\nHP Anywhere 9\\nSince 11.7: Red Hat Enterprise Linux 7.9\\n11.3-11.6 only: Red Hat Enterprise Linux 7.7-7.9\\n11.1-11.2 only: Red Hat Enterprise Linux 7.6-7.9\\n11.0 only: Red Hat Enterprise Linux 7.6-7.8\\nUbuntu 14.04 LTS/16.04/18.04 12 LTS\\nVNC\\nHP Anywhere 9\\n11.0-11.4 only: NVIDIA vGPU for XenServer 7.0 5\\nCitrix XenServer 7.0\\nNVIDIA vGPU\\nPass-through GPU\\nM6, M10, M60\\nP4, P6, P40, P100,\\nP100 12GB\\nV100\\nRTX 6000, RTX 8000 7\\n11.3, 11.4 only: Windows 10 21H1 8\\n11.0-11.2 only: Windows 10 2004 8\\nCitrix Virtual Apps and Desktops\\nHP Anywhere 9\\nWindows Server 2012 R2 10\\nWindows Server 2016 1607, 1709\\nCitrix Virtual Apps and Desktops\\nRDSH\\nHP Anywhere 9\\n11.3, 11.4 only: Red Hat Enterprise Linux 7.7-7.9\\n11.1-11.2 only: Red Hat Enterprise Linux 7.6-7.9\\n11.0 only: Red Hat Enterprise Linux 7.6-7.8\\nUbuntu 14.04 LTS/16.04/18.04 LTS\\nVNC\\nHP Anywhere 9\\nLinux with KVM Support\\nNVIDIA vGPU software is supported on Linux with KVM platforms only by specific hypervisor software vendors. For information about which NVIDIA vGPU software releases and hypervisor software releases are supported, consult the documentation from your hypervisor vendor.']]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_chunks = [[i[\"url\"], i[\"chunk\"]] for i in res]\n",
    "\n",
    "res_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "API_URL = \"https://api-inference.huggingface.co/models/BAAI/bge-m3\"\n",
    "headers = {\"Authorization\": \"Bearer \"+str(os.getenv('HUGGINGFACE_TOKEN'))}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\n",
    "\t\"inputs\": {\n",
    "\t\"source_sentence\": question,\n",
    "\t\"sentences\":list([i[1] for i in res_chunks])\n",
    "},\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5938552618026733,\n",
       " 0.6311699151992798,\n",
       " 0.6791618466377258,\n",
       " 0.6869580149650574,\n",
       " 0.5938552618026733,\n",
       " 0.6657265424728394,\n",
       " 0.6852164268493652,\n",
       " 0.6842777729034424,\n",
       " 0.6716938018798828,\n",
       " 0.6732029914855957]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the CUDA version suppoorted by NVIDIA RTX\n",
      "Context: https://docs.nvidia.com/vgpu/9.0/grid-vgpu-release-notes-red-hat-el-kvm/index.html - Source. \n",
      "NVIDIA CUDA Toolkit Version Support\n",
      "The releases in this release family of NVIDIA vGPU software support NVIDIA CUDA Toolkit 10.1 Update 1. For more information about NVIDIA CUDA Toolkit, see CUDA Toolkit 10.1 Documentation . Note:\n",
      "If you are using NVIDIA vGPU software with CUDA on Linux, avoid conflicting installation methods by installing CUDA from a distribution-independent runfile package. Do not install CUDA from distribution-specific RPM or Deb package. To ensure that the NVIDIA vGPU software graphics driver is not overwritten when CUDA is installed, deselect the CUDA driver when selecting the CUDA components to install. For more information, see NVIDIA CUDA Installation Guide for Linux . 2.5. Multiple vGPU Support\n",
      "To support applications and workloads that are compute or graphics intensive, multiple vGPUs can be added to a single VM. The assignment of more than one vGPU to a VM is supported only on a subset of vGPUs and Red Hat Enterprise Linux with KVM releases. Supported vGPUs\n",
      "Only Q-series and C-series vGPUs that are allocated all of the physical GPU's frame buffer are supported. GPU Architecture\n",
      "Board\n",
      "vGPU\n",
      "Turing\n",
      "Tesla T4\n",
      "T4-16Q\n",
      "T4-16C\n",
      "Quadro RTX 6000\n",
      "RTX6000-24Q\n",
      "Quadro RTX 8000\n",
      "RTX8000-48Q\n",
      "Volta\n",
      "Tesla V100 SXM2 32GB\n",
      "V100DX-32Q\n",
      "V100D-32C\n",
      "Tesla V100 PCIe 32GB\n",
      "V100D-32Q\n",
      "V100D-32C\n",
      "Tesla V100 SXM2\n",
      "V100X-16Q\n",
      "V100X-16C\n",
      "Tesla V100 PCIe\n",
      "V100-16Q\n",
      "V100-16C\n",
      "Tesla V100 FHHL\n",
      "V100L-16Q\n",
      "V100L-16C\n",
      "Pascal\n",
      "Tesla P100 SXM2\n",
      "P100X-16Q\n",
      "P100X-16C\n",
      "Tesla P100 PCIe 16GB\n",
      "P100-16Q\n",
      "P100-16C\n",
      "Tesla P100 PCIe 12GB\n",
      "P100C-12Q\n",
      "P100C-12C\n",
      "Tesla P40\n",
      "P40-24Q\n",
      "P40-24C\n",
      "Tesla P6\n",
      "P6-16Q\n",
      "P6-16C\n",
      "Tesla P4\n",
      "P4-8Q\n",
      "P4-8C\n",
      "Maxwell\n",
      "Tesla M60\n",
      "M60-8Q\n",
      "Tesla M10\n",
      "M10-8Q\n",
      "Tesla M6\n",
      "M6-8Q\n",
      "Maximum vGPUs per VM\n",
      "NVIDIA vGPU software supports up to a maximum of 16 vGPUs per VM on Red Hat Enterprise Linux with KVM. Supported Hypervisor Releases\n",
      "Red Hat Enterprise Linux with KVM 7.6 and 7.5 only.\n"
     ]
    }
   ],
   "source": [
    "print(\"Question: \"+question)\n",
    "print(\"Context: \"+\" - Source. \\n\".join(res_chunks[output.index(max(output))]) if len(output)==len(res_chunks) else \"The retreival system is still loading, Please try after 2 minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\nSource: \".join(res_chunks[output.index(max(output))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a test context for the `@mui/material` library.\\n\\n## Installation\\n\\n```sh\\nnpm install @mui/material\\n```\\n\\n## Usage\\n\\n```jsx\\nimport React from \\'react\\';\\nimport { Button } from \\'@mui/material\\';\\n\\nfunction App() {\\n  return (\\n    <div className=\"App\">\\n      <Button variant=\"contained\" color=\"primary\">\\n        Hello World\\n      </Button>\\n    </div>\\n  );\\n}\\n\\nexport default App;\\n```\\n\\n## Documentation\\n\\n- [Material-UI](https://material-ui.com/)\\n- [Material Design](https://material.io/)'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import json\n",
    "\n",
    "\n",
    "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "llm_client = InferenceClient(\n",
    "    model=repo_id,\n",
    "    timeout=120,\n",
    "    token=str(os.getenv('HUGGINGFACE_TOKEN'))\n",
    ")\n",
    "\n",
    "\n",
    "def call_llm(inference_client: InferenceClient, prompt: str):\n",
    "    response = inference_client.post(\n",
    "        json={\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\"max_new_tokens\": 300},\n",
    "            \"task\": \"text-generation\",\n",
    "        },\n",
    "    )\n",
    "    return json.loads(response.decode())[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "call_llm(llm_client, \"This is a test context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Your task is to answer a question given a context.\n",
    "Question asked: \"\"\"+ question+\"\"\"\n",
    "Context provided to you from NVIDIA Documentation site: \"\"\"+ context[:500]+\"\"\"\n",
    "You always provide the URL source for context right after the answer.\n",
    "Your answer is:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = call_llm(llm_client,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The NVIDIA RTX supports CUDA Toolkit version 10.1 Update 1. Source: https://docs.nvidia.com/vgpu/9.0/grid-vgpu-release-notes-red-hat-el-kvm/index.html#cuda-toolkit-version-support'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(res.split(\"\\n\")[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bharathraj/anaconda3/envs/nemo_env/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "## Complete functionality\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from pymilvus import MilvusClient, DataType\n",
    "import os\n",
    "import requests\n",
    "from huggingface_hub import InferenceClient\n",
    "import json\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "\n",
    "CLUSTER_ENDPOINT = \"https://in03-6f08bdb85fec6ea.api.gcp-us-west1.zillizcloud.com\"\n",
    "TOKEN = os.getenv('ZILLIZ_TOKEN')\n",
    "\n",
    "client = MilvusClient(\n",
    "    uri=CLUSTER_ENDPOINT,\n",
    "    token=TOKEN \n",
    ")\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/BAAI/bge-m3\"\n",
    "headers = {\"Authorization\": \"Bearer \"+str(os.getenv('HUGGINGFACE_TOKEN'))}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\n",
    "\t\"inputs\": {\n",
    "\t\"source_sentence\": [\"placeholder_to_load_the_model\"],\n",
    "\t\"sentences\":[\"placeholder_to_load_the_model\", \"placeholder_to_load_the_model\"],\n",
    "},\n",
    "})\n",
    "\n",
    "\n",
    "llm_client = InferenceClient(\n",
    "    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    timeout=120,\n",
    "    token=str(os.getenv('HUGGINGFACE_TOKEN'))\n",
    ")\n",
    "\n",
    "\n",
    "def call_llm(inference_client: InferenceClient, prompt: str):\n",
    "    response = inference_client.post(\n",
    "        json={\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\"max_new_tokens\": 1000},\n",
    "            \"task\": \"text-generation\",\n",
    "        },\n",
    "    )\n",
    "    return json.loads(response.decode())[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "call_llm(llm_client, \"This is a test context\")\n",
    "\n",
    "def chatbot_chat(question):\n",
    "    embedded_query = hf_embeddings.embed_query(question)\n",
    "\n",
    "    client.load_collection(\n",
    "    collection_name=\"cuda_data\",\n",
    "    )\n",
    "\n",
    "    res = client.search(\n",
    "        collection_name=\"cuda_data\", \n",
    "        data=[embedded_query],\n",
    "        limit=10, \n",
    "        search_params={\"metric_type\": \"COSINE\", \"params\": {}}\n",
    "    )\n",
    "    ids = [i[\"id\"] for i in res[0]]\n",
    "    res = client.get(\n",
    "        collection_name=\"cuda_data\",\n",
    "        ids=ids\n",
    "    )\n",
    "\n",
    "    res_chunks = [[i[\"url\"], i[\"chunk\"]] for i in res]\n",
    "\n",
    "    output = query({\n",
    "        \"inputs\": {\n",
    "        \"source_sentence\": question,\n",
    "        \"sentences\":list([i[1] for i in res_chunks])\n",
    "    },\n",
    "    })\n",
    "\n",
    "    context = \"\\nSource: \".join(res_chunks[output.index(max(output))])\n",
    "    \n",
    "    prompt = \"\"\"\n",
    "Your task is to answer a question given a context.\n",
    "Question asked: \"\"\"+ question+\"\"\"\n",
    "Context provided to you from NVIDIA Documentation site: \"\"\"+ context[:1500]+\"\"\"\n",
    "You always provide the URL source for context right after the answer.\n",
    "Your answer is:\n",
    "\"\"\"\n",
    "    res = call_llm(llm_client,prompt)\n",
    "    return \" \".join(res.split(\"\\n\")[-2:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your answer is: The new features of CUDA 11.6 include support for VS2022 as a host compiler, new instructions for bit mask creation (BMSK) and sign extension (SZEXT) in public PTX, and unused kernel optimization enabled by default.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_chat(\"What are the new features of CUDA 11.6?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA Performance Primitives (NPP) is a library of functions for performing image and signal processing operations. It is designed to provide high performance on NVIDIA GPUs. NPP provides a wide range of functions for image filtering, color space conversion, morphological operations, and more. It is intended to be used as a building block for higher-level image processing and computer vision applications. Source: https://docs.nvidia.com/cuda/npp/introduction.html'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_chat(\"What is NVIDIA NPP?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemo_env",
   "language": "python",
   "name": "nemo_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
